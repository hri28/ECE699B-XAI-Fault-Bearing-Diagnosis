{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGF9VoCiwibr",
        "outputId": "feb18f75-ddb2-4078-fc74-eeaca73b0f93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/BearingProject/scripts')\n"
      ],
      "metadata": {
        "id": "-JuDZmQgwpvd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from SDA_data_utils import (\n",
        "    load_cwru_signals, load_paderborn_signals,\n",
        "    segment_signals, maybe_resample_list\n",
        ")\n",
        "\n",
        "from SDA_model_utils import build_cnn1d\n"
      ],
      "metadata": {
        "id": "3hShRmdWwstC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/BearingProject/scripts/SDA_data_utils.py"
      ],
      "metadata": {
        "id": "iK99zXyJ3dac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfYNeNcXuqeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5C2BFwDCvgn4"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers, losses\n",
        "\n",
        "from SDA_data_utils import (\n",
        "    load_cwru_signals, load_paderborn_signals,\n",
        "    segment_signals, maybe_resample_list\n",
        ")\n",
        "\n",
        "from SDA_model_utils import build_cnn1d\n",
        "\n",
        "# ==========================================\n",
        "# CONFIG\n",
        "# ==========================================\n",
        "SOURCE_KEY = \"12K3HP\"\n",
        "TARGET_COND = \"Condition2\"\n",
        "\n",
        "KFOLD = 5\n",
        "PRETRAIN_EPOCHS = 200\n",
        "BATCH_SIZE = 64\n",
        "PATIENCE = 50\n",
        "\n",
        "USE_TARGET_FRACTION = 1.0\n",
        "SEED = 42\n",
        "VERBOSE = True\n",
        "\n",
        "\n",
        "INTERVAL_LENGTH = 320\n",
        "SAMPLES_PER_BLOCK = 1600\n",
        "\n",
        "# Explainability settings\n",
        "NUM_EXPLAIN_PER_CLASS = 5\n",
        "\n",
        "OUTPUT_ROOT = \"12K3HP_COND1\"\n",
        "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ==========================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ==========================================\n",
        "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_training_history(history_log, fold_dir, fold):\n",
        "    epochs = [h[\"epoch\"] for h in history_log]\n",
        "    train_loss = [h[\"train_loss\"] for h in history_log]\n",
        "    val_acc = [h[\"val_acc\"] for h in history_log]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_loss, label=\"Train Loss\")\n",
        "    plt.plot(epochs, val_acc, label=\"Val Acc\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.title(f\"Fold {fold} Training History\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(fold_dir, f\"Fold{fold}_History.png\"))\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_conv_layer(model):\n",
        "    for layer in reversed(model.layers):\n",
        "        if isinstance(layer, tf.keras.layers.Conv1D):\n",
        "            return layer.name\n",
        "    raise ValueError(\"Model has no Conv1D layers for Grad-CAM.\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# EXPLAINABILITY â€” GRAD-CAM\n",
        "# ==========================================\n",
        "def compute_gradcam(model, sample, pred_index, last_conv_name=\"conv1d_3\"):\n",
        "    \"\"\"Compute 1D Grad-CAM \"\"\"\n",
        "    last_conv_layer = model.get_layer(last_conv_name)\n",
        "\n",
        "    # Create a gradient model\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=[last_conv_layer.output, model.output]\n",
        "    )\n",
        "\n",
        "    # Convert sample to tensor\n",
        "    sample_tensor = tf.cast(sample, tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get conv output and predictions in one forward pass\n",
        "        conv_output, preds = grad_model(sample_tensor, training=False)\n",
        "\n",
        "        # Get the score for the predicted class\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # Compute gradients of the class score with respect to conv output\n",
        "    grads = tape.gradient(class_channel, conv_output)\n",
        "\n",
        "    # Global average pooling on gradients\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
        "\n",
        "    # Get theconv output as numpy\n",
        "    conv_output = conv_output[0].numpy()  # (T, C)\n",
        "    pooled_grads = pooled_grads.numpy()   # (C,)\n",
        "\n",
        "    # Weight each channel by the corresponding gradient\n",
        "    for i in range(conv_output.shape[-1]):\n",
        "        conv_output[:, i] *= pooled_grads[i]\n",
        "\n",
        "    # Create heatmap by averaging across channels\n",
        "    heatmap = np.mean(conv_output, axis=-1)\n",
        "\n",
        "    # ReLU - only keep positive contributions\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    if np.max(heatmap) > 0:\n",
        "        heatmap = heatmap / np.max(heatmap)\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "def plot_gradcam(sample_1d, heatmap, true_label, pred_label,\n",
        "                 class_names, save_path):\n",
        "    T = len(sample_1d)\n",
        "    time = np.arange(T)\n",
        "\n",
        "    heatmap_resized = np.interp(\n",
        "        np.linspace(0, len(heatmap) - 1, T),\n",
        "        np.arange(len(heatmap)),\n",
        "        heatmap\n",
        "    )\n",
        "\n",
        "    fig, axs = plt.subplots(3,1,figsize=(14,9))\n",
        "\n",
        "    # Raw signal\n",
        "    axs[0].plot(time, sample_1d, linewidth=0.7)\n",
        "    axs[0].set_title(f\"Signal â€” True: {class_names[true_label]}, Pred: {class_names[pred_label]}\")\n",
        "    axs[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Heatmap\n",
        "    axs[1].imshow(heatmap_resized.reshape(1,-1), cmap=\"jet\",\n",
        "                  aspect=\"auto\", extent=[0,T,0,1])\n",
        "    axs[1].set_title(\"Grad-CAM Heatmap (Red = high importance)\")\n",
        "    axs[1].set_yticks([])\n",
        "\n",
        "    # Overlay\n",
        "    axs[2].plot(time, sample_1d, alpha=0.7)\n",
        "    axs[2].imshow(heatmap_resized.reshape(1,-1), cmap=\"jet\",\n",
        "                  alpha=0.4, aspect=\"auto\",\n",
        "                  extent=[0,T, sample_1d.min(), sample_1d.max()])\n",
        "    axs[2].set_title(\"Overlay: Signal + Grad-CAM\")\n",
        "    axs[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# EXPLAINABILITY â€” IMPROVED LRP (SmoothGrad + IG)\n",
        "# ==========================================\n",
        "class ImprovedLRP:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def compute_relevance(self, sample, pred_index,\n",
        "                          num_samples=30, noise_level=0.15):\n",
        "        baseline = np.zeros_like(sample)\n",
        "        T = sample.shape[1]\n",
        "\n",
        "        signal_range = np.max(sample) - np.min(sample)\n",
        "        stdev = noise_level * signal_range if signal_range > 0 else 0.01\n",
        "\n",
        "        total_gradients = np.zeros_like(sample)\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            noise = np.random.normal(0, stdev, sample.shape)\n",
        "            noisy_input = sample + noise\n",
        "\n",
        "            alphas = np.linspace(0, 1, 20)\n",
        "            path_grads = []\n",
        "\n",
        "            for a in alphas:\n",
        "                x = baseline + a * (noisy_input - baseline)\n",
        "                with tf.GradientTape() as tape:\n",
        "                    x_tf = tf.Variable(x, dtype=tf.float32)\n",
        "                    preds = self.model(x_tf)\n",
        "                    target = preds[:, pred_index]\n",
        "\n",
        "                grads = tape.gradient(target, x_tf)\n",
        "                if grads is not None:\n",
        "                    path_grads.append(grads.numpy())\n",
        "\n",
        "            if len(path_grads) > 0:\n",
        "                avg_grad = np.mean(path_grads, axis=0)\n",
        "                total_gradients += avg_grad\n",
        "\n",
        "        smooth_grads = total_gradients / num_samples\n",
        "\n",
        "        relevance = (sample - baseline) * smooth_grads\n",
        "        relevance = relevance[0,:,0]  # flatten to 1D\n",
        "\n",
        "        # Gaussian smoothing\n",
        "        from scipy.ndimage import gaussian_filter1d\n",
        "        relevance_smooth = gaussian_filter1d(relevance, sigma=2.5)\n",
        "\n",
        "        return relevance_smooth\n",
        "\n",
        "\n",
        "def plot_lrp(sample_1d, relevance, true_label, pred_label,\n",
        "             class_names, save_path):\n",
        "    \"\"\"Plot sample, LRP relevance, absolute relevance, and colored overlay.\"\"\"\n",
        "    T = len(sample_1d)\n",
        "    time = np.arange(T)\n",
        "\n",
        "    abs_rel = np.abs(relevance)\n",
        "    top_idx = np.argsort(abs_rel)[-5:]  # top 5 points\n",
        "\n",
        "    fig, axs = plt.subplots(4,1,figsize=(14,11))\n",
        "\n",
        "    # Raw signal\n",
        "    axs[0].plot(time, sample_1d)\n",
        "    axs[0].set_title(f\"Signal â€” True: {class_names[true_label]}, Pred: {class_names[pred_label]}\")\n",
        "    axs[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Smoothed relevance\n",
        "    axs[1].plot(time, relevance, color='red')\n",
        "    axs[1].set_title(\"LRP Relevance (Smoothed)\")\n",
        "    axs[1].axhline(0, linestyle=\"--\", alpha=0.4)\n",
        "    axs[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Absolute relevance\n",
        "    axs[2].plot(time, abs_rel, color='darkred')\n",
        "    axs[2].fill_between(time, abs_rel, alpha=0.4)\n",
        "    axs[2].scatter(time[top_idx], abs_rel[top_idx],\n",
        "                   c='yellow', edgecolors='black', s=80, label=\"Top Points\")\n",
        "    axs[2].legend()\n",
        "    axs[2].set_title(\"|Relevance| Strongest Regions\")\n",
        "    axs[2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Overlay scatter\n",
        "    rel_norm = (relevance - relevance.min()) / (relevance.max() - relevance.min() + 1e-9)\n",
        "    axs[3].scatter(time, sample_1d, c=rel_norm, cmap=\"RdYlBu_r\", s=4)\n",
        "    axs[3].set_title(\"Signal with LRP Coloring\")\n",
        "    axs[3].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "bUXWEqcEvr0K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# GENERATE EXPLAINABILITY FOR ONE FOLD\n",
        "# ==========================================\n",
        "def generate_explainability_for_fold(model, X, y, class_names, save_dir, prefix):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    gradcam_dir = os.path.join(save_dir, \"GradCAM\")\n",
        "    lrp_dir = os.path.join(save_dir, \"LRP\")\n",
        "    os.makedirs(gradcam_dir, exist_ok=True)\n",
        "    os.makedirs(lrp_dir, exist_ok=True)\n",
        "\n",
        "        # Debug: Check label values\n",
        "    print(f\"Unique labels in y: {np.unique(y)}\")\n",
        "    print(f\"Class names length: {len(class_names)}\")\n",
        "    print(f\"Class names: {class_names}\")\n",
        "\n",
        "    preds = np.argmax(model.predict(X, verbose=0), axis=1)\n",
        "\n",
        "    # Debug: Check prediction values\n",
        "    print(f\"Unique predictions: {np.unique(preds)}\")\n",
        "\n",
        "    lrp = ImprovedLRP(model)\n",
        "\n",
        "    for cls in np.unique(y):\n",
        "        idx_cls = np.where(y == cls)[0]\n",
        "\n",
        "        # pick 5 samples\n",
        "        chosen = idx_cls[:NUM_EXPLAIN_PER_CLASS]\n",
        "\n",
        "        for i, idx in enumerate(chosen):\n",
        "            sample = X[idx:idx+1]\n",
        "            sample_1d = sample[0,:,0]\n",
        "            true_lbl = int(y[idx])  # Ensure it's an int\n",
        "            pred_lbl = int(preds[idx])  # Ensure it's an int\n",
        "\n",
        "            # -----------------------\n",
        "            # GRAD-CAM\n",
        "            # -----------------------\n",
        "            last_conv_name = get_last_conv_layer(model)\n",
        "            heatmap = compute_gradcam(model, sample, pred_lbl, last_conv_name)\n",
        "            save_gc = os.path.join(\n",
        "                gradcam_dir,\n",
        "                f\"{prefix}_class{cls}_sample{i+1}_pred{pred_lbl}.png\"\n",
        "            )\n",
        "            plot_gradcam(sample_1d, heatmap, true_lbl, pred_lbl,\n",
        "                         class_names, save_gc)\n",
        "\n",
        "            # -----------------------\n",
        "            # LRP\n",
        "            # -----------------------\n",
        "            relevance = lrp.compute_relevance(sample, pred_lbl)\n",
        "            save_lrp = os.path.join(\n",
        "                lrp_dir,\n",
        "                f\"{prefix}_class{cls}_sample{i+1}_pred{pred_lbl}.png\"\n",
        "            )\n",
        "            plot_lrp(sample_1d, relevance, true_lbl, pred_lbl,\n",
        "                     class_names, save_lrp)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# MAIN TRAINING WITH 5-FOLD CV\n",
        "# ==========================================\n",
        "def train_no_da_kfold(src_signals, src_labels, tgt_signals, tgt_labels,\n",
        "                      src_rates, tgt_rates):\n",
        "    # Output folder\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    base_dir = os.path.join(OUTPUT_ROOT, f\"NoDA_{timestamp}\")\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    # Resample to lowest source rate\n",
        "    target_fs = min(src_rates)\n",
        "    src_res = maybe_resample_list(src_signals, src_rates, target_fs)\n",
        "    tgt_res = maybe_resample_list(tgt_signals, tgt_rates, target_fs)\n",
        "\n",
        "    X_src, y_src = segment_signals(src_res, src_labels,\n",
        "                                   INTERVAL_LENGTH, SAMPLES_PER_BLOCK)\n",
        "    X_tgt, y_tgt = segment_signals(tgt_res, tgt_labels,\n",
        "                                   INTERVAL_LENGTH, SAMPLES_PER_BLOCK)\n",
        "\n",
        "    print(\"Source samples:\", len(X_src))\n",
        "    print(\"Target samples:\", len(X_tgt))\n",
        "\n",
        "    # 5-fold split on SOURCE only\n",
        "    kfold = KFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
        "\n",
        "    fold_results_src = []\n",
        "    fold_results_tgt = []\n",
        "\n",
        "    class_names = [\"Healthy\", \"OR_Fault\", \"IR_Fault\"]\n",
        "\n",
        "    X_t_test = X_tgt  # Use all target data\n",
        "    y_t_test = y_tgt\n",
        "    print(f\"Target test samples: {len(X_t_test)}\")\n",
        "    fold = 1\n",
        "    for train_idx, val_idx in kfold.split(X_src, y_src):\n",
        "        print(f\"\\n========== FOLD {fold}/{KFOLD} ==========\")\n",
        "        fold_dir = os.path.join(base_dir, f\"Fold_{fold}\")\n",
        "        os.makedirs(fold_dir, exist_ok=True)\n",
        "\n",
        "        # Split source into train/val\n",
        "        Xs_train, Xs_val = X_src[train_idx], X_src[val_idx]\n",
        "        ys_train, ys_val = y_src[train_idx], y_src[val_idx]\n",
        "\n",
        "        # Build CNN\n",
        "        num_classes = len(np.unique(y_src))\n",
        "        model = build_cnn1d(X_src.shape[1:], num_classes)\n",
        "        opt = optimizers.Adam(1e-4)\n",
        "        loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "        best_val = 0\n",
        "        epochs_no_improve = 0\n",
        "        history_log = []\n",
        "        best_path = os.path.join(fold_dir, \"best_model.h5\")\n",
        "\n",
        "        # -------------------------------\n",
        "        # TRAINING LOOP (NO DA)\n",
        "        # -------------------------------\n",
        "        for ep in range(PRETRAIN_EPOCHS):\n",
        "\n",
        "            # Shuffle source\n",
        "            idx = np.random.permutation(len(Xs_train))\n",
        "            Xs_train_sh = Xs_train[idx]\n",
        "            ys_train_sh = ys_train[idx]\n",
        "\n",
        "            num_batches = max(len(Xs_train_sh) // BATCH_SIZE, 1)\n",
        "            train_loss_accum = 0\n",
        "\n",
        "            for b in range(num_batches):\n",
        "                start = b * BATCH_SIZE\n",
        "                end = start + BATCH_SIZE\n",
        "                xb = Xs_train_sh[start:end]\n",
        "                yb = ys_train_sh[start:end]\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    logits = model(xb, training=True)\n",
        "                    loss = loss_fn(yb, logits)\n",
        "\n",
        "                grads = tape.gradient(loss, model.trainable_variables)\n",
        "                opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "                train_loss_accum += float(loss.numpy())\n",
        "\n",
        "            # Validation\n",
        "            val_logits = model.predict(Xs_val, verbose=0)\n",
        "            val_preds = np.argmax(val_logits, axis=1)\n",
        "            val_acc = accuracy_score(ys_val, val_preds)\n",
        "            val_loss = np.mean(\n",
        "                losses.sparse_categorical_crossentropy(ys_val, val_logits).numpy()\n",
        "            )\n",
        "\n",
        "            history_log.append({\n",
        "                \"epoch\": ep + 1,\n",
        "                \"train_loss\": train_loss_accum / num_batches,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"val_loss\": val_loss\n",
        "            })\n",
        "\n",
        "            print(f\"Epoch {ep+1}: TrainLoss={train_loss_accum/num_batches:.4f}  ValAcc={val_acc:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_acc > best_val:\n",
        "                best_val = val_acc\n",
        "                model.save(best_path)\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        model = tf.keras.models.load_model(best_path)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(1e-4),\n",
        "            loss=losses.SparseCategoricalCrossentropy(),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Warm up the model with a prediction\n",
        "        _ = model.predict(np.zeros((1, X_src.shape[1], 1), dtype=np.float32), verbose=0)\n",
        "\n",
        "\n",
        "        # -------------------------------\n",
        "        # EVALUATE â€” SOURCE VAL\n",
        "        # -------------------------------\n",
        "        y_pred_val = np.argmax(model.predict(Xs_val, verbose=0), axis=1)\n",
        "        fold_results_src.append([\n",
        "            accuracy_score(ys_val, y_pred_val),\n",
        "            precision_score(ys_val, y_pred_val, average=\"macro\", zero_division=0),\n",
        "            recall_score(ys_val, y_pred_val, average=\"macro\", zero_division=0),\n",
        "            f1_score(ys_val, y_pred_val, average=\"macro\", zero_division=0)\n",
        "        ])\n",
        "        plot_confusion_matrix(\n",
        "            ys_val, y_pred_val,\n",
        "            f\"Fold {fold} â€” Source ConfMat\",\n",
        "            os.path.join(fold_dir, \"ConfMat_Source.png\")\n",
        "        )\n",
        "        plot_training_history(history_log, fold_dir, fold)\n",
        "\n",
        "        # -------------------------------\n",
        "        # EVALUATE â€” TARGET TEST\n",
        "        # -------------------------------\n",
        "        y_pred_tgt = np.argmax(model.predict(X_t_test, verbose=0), axis=1)\n",
        "        fold_results_tgt.append([\n",
        "            accuracy_score(y_t_test, y_pred_tgt),\n",
        "            precision_score(y_t_test, y_pred_tgt, average=\"macro\", zero_division=0),\n",
        "            recall_score(y_t_test, y_pred_tgt, average=\"macro\", zero_division=0),\n",
        "            f1_score(y_t_test, y_pred_tgt, average=\"macro\", zero_division=0)\n",
        "        ])\n",
        "        plot_confusion_matrix(\n",
        "            y_t_test, y_pred_tgt,\n",
        "            f\"Fold {fold} â€” Target ConfMat\",\n",
        "            os.path.join(fold_dir, \"ConfMat_Target.png\")\n",
        "        )\n",
        "\n",
        "        # -------------------------------\n",
        "        # TSNE (only fold 5 like supervisor)\n",
        "        # -------------------------------\n",
        "        if fold == KFOLD:\n",
        "            feat_model = models.Model(inputs=model.input,\n",
        "                                      outputs=model.layers[-3].output)\n",
        "            src_feats = feat_model.predict(Xs_val)\n",
        "            tgt_feats = feat_model.predict(X_t_test)\n",
        "\n",
        "            tsne = TSNE(n_components=2, random_state=SEED)\n",
        "            final_feats = tsne.fit_transform(np.vstack([src_feats, tgt_feats]))\n",
        "            labels = np.array([0]*len(src_feats) + [1]*len(tgt_feats))\n",
        "\n",
        "            plt.figure()\n",
        "            plt.scatter(final_feats[labels==0,0], final_feats[labels==0,1],\n",
        "                        label=\"Source\", alpha=0.6)\n",
        "            plt.scatter(final_feats[labels==1,0], final_feats[labels==1,1],\n",
        "                        label=\"Target\", alpha=0.6)\n",
        "            plt.legend()\n",
        "            plt.title(\"Fold 5 â€” t-SNE Source vs Target\")\n",
        "            plt.savefig(os.path.join(fold_dir, \"TSNE.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # -------------------------------\n",
        "        # EXPLAINABILITY (only Fold 5)\n",
        "        # -------------------------------\n",
        "        if fold == KFOLD:\n",
        "            print(\"\\nGenerating explainability for Fold 5...\")\n",
        "            exp_dir_src = os.path.join(fold_dir, \"Explainability_Source\")\n",
        "            exp_dir_tgt = os.path.join(fold_dir, \"Explainability_Target\")\n",
        "\n",
        "            generate_explainability_for_fold(\n",
        "                model, Xs_val, ys_val, class_names,\n",
        "                exp_dir_src,\n",
        "                prefix=\"Source\"\n",
        "            )\n",
        "            generate_explainability_for_fold(\n",
        "                model, X_t_test, y_t_test, class_names,\n",
        "                exp_dir_tgt,\n",
        "                prefix=\"Target\"\n",
        "            )\n",
        "\n",
        "        fold += 1\n",
        "   # ==========================================\n",
        "    # CREATE DATAFRAMES FIRST\n",
        "    # ==========================================\n",
        "    df_src = pd.DataFrame(fold_results_src,\n",
        "                          columns=[\"Acc\",\"Prec\",\"Rec\",\"F1\"])\n",
        "    df_tgt = pd.DataFrame(fold_results_tgt,\n",
        "                          columns=[\"Acc\",\"Prec\",\"Rec\",\"F1\"])\n",
        "    df_src[\"Type\"] = \"Source\"\n",
        "    df_tgt[\"Type\"] = \"Target\"\n",
        "    df_src[\"Fold\"] = [f\"Fold_{i+1}\" for i in range(len(df_src))]\n",
        "    df_tgt[\"Fold\"] = [f\"Fold_{i+1}\" for i in range(len(df_tgt))]\n",
        "\n",
        "    df_final = pd.concat([df_src, df_tgt], ignore_index=True)\n",
        "\n",
        "    # ==========================================\n",
        "    # PRINT SUMMARY TABLE\n",
        "    # ==========================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL CROSS-DOMAIN RESULTS SUMMARY (5-FOLD CROSS-VALIDATION)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Calculate means and stds\n",
        "    src_mean = df_src[[\"Acc\",\"Prec\",\"Rec\",\"F1\"]].mean()\n",
        "    src_std = df_src[[\"Acc\",\"Prec\",\"Rec\",\"F1\"]].std()\n",
        "    tgt_mean = df_tgt[[\"Acc\",\"Prec\",\"Rec\",\"F1\"]].mean()\n",
        "    tgt_std = df_tgt[[\"Acc\",\"Prec\",\"Rec\",\"F1\"]].std()\n",
        "\n",
        "    # Print Source Results\n",
        "    print(\"\\nðŸ“Š SOURCE DOMAIN - CWRU (Validation Accuracy):\")\n",
        "    print(\"   (Model trained on CWRU train set, evaluated on CWRU validation set)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Fold':<10} {'Accuracy':<15} {'Precision':<15} {'Recall':<15} {'F1-Score':<15}\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, row in df_src.iterrows():\n",
        "        print(f\"{row['Fold']:<10} {row['Acc']*100:>6.2f}%{'':<7} {row['Prec']*100:>6.2f}%{'':<7} \"\n",
        "              f\"{row['Rec']*100:>6.2f}%{'':<7} {row['F1']*100:>6.2f}%\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'MEAN':<10} {src_mean['Acc']*100:>6.2f}%{'':<7} {src_mean['Prec']*100:>6.2f}%{'':<7} \"\n",
        "          f\"{src_mean['Rec']*100:>6.2f}%{'':<7} {src_mean['F1']*100:>6.2f}%\")\n",
        "    print(f\"{'STD':<10} {src_std['Acc']*100:>6.2f}%{'':<7} {src_std['Prec']*100:>6.2f}%{'':<7} \"\n",
        "          f\"{src_std['Rec']*100:>6.2f}%{'':<7} {src_std['F1']*100:>6.2f}%\")\n",
        "\n",
        "    # Print Target Results\n",
        "    print(\"\\nðŸ“Š TARGET DOMAIN - Paderborn (Test Accuracy - CROSS-DOMAIN):\")\n",
        "    print(\"   (Model trained on CWRU, evaluated on Paderborn test set)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Fold':<10} {'Accuracy':<15} {'Precision':<15} {'Recall':<15} {'F1-Score':<15}\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, row in df_tgt.iterrows():\n",
        "        print(f\"{row['Fold']:<10} {row['Acc']*100:>6.2f}%{'':<7} {row['Prec']*100:>6.2f}%{'':<7} \"\n",
        "              f\"{row['Rec']*100:>6.2f}%{'':<7} {row['F1']*100:>6.2f}%\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'MEAN':<10} {tgt_mean['Acc']*100:>6.2f}%{'':<7} {tgt_mean['Prec']*100:>6.2f}%{'':<7} \"\n",
        "          f\"{tgt_mean['Rec']*100:>6.2f}%{'':<7} {tgt_mean['F1']*100:>6.2f}%\")\n",
        "    print(f\"{'STD':<10} {tgt_std['Acc']*100:>6.2f}%{'':<7} {tgt_std['Prec']*100:>6.2f}%{'':<7} \"\n",
        "          f\"{tgt_std['Rec']*100:>6.2f}%{'':<7} {tgt_std['F1']*100:>6.2f}%\")\n",
        "\n",
        "    # Domain Gap Analysis\n",
        "    print(\"\\nðŸ“‰ CROSS-DOMAIN TRANSFER PERFORMANCE:\")\n",
        "    print(\"-\" * 80)\n",
        "    acc_gap = (src_mean['Acc'] - tgt_mean['Acc']) * 100\n",
        "    f1_gap = (src_mean['F1'] - tgt_mean['F1']) * 100\n",
        "    transfer_rate = (tgt_mean['Acc'] / src_mean['Acc']) * 100 if src_mean['Acc'] > 0 else 0\n",
        "    print(f\"Source Domain (CWRU) Accuracy:     {src_mean['Acc']*100:.2f}% Â± {src_std['Acc']*100:.2f}%\")\n",
        "    print(f\"Target Domain (Paderborn) Accuracy: {tgt_mean['Acc']*100:.2f}% Â± {tgt_std['Acc']*100:.2f}%\")\n",
        "    print(f\"Accuracy Drop:                      {acc_gap:.2f}%\")\n",
        "    print(f\"Transfer Success Rate:              {transfer_rate:.2f}%\")\n",
        "    print(f\"F1-Score Drop:                      {f1_gap:.2f}%\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nðŸ’¡ Interpretation:\")\n",
        "    print(f\"   - High source accuracy ({src_mean['Acc']*100:.2f}%) shows model learned CWRU data well\")\n",
        "    print(f\"   - Lower target accuracy ({tgt_mean['Acc']*100:.2f}%) shows domain shift challenge\")\n",
        "    print(f\"   - The {acc_gap:.2f}% drop indicates the need for domain adaptation methods\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # ==========================================\n",
        "    # SAVE EXCEL\n",
        "    # ==========================================\n",
        "    df_final.to_excel(os.path.join(base_dir, \"Summary.xlsx\"), index=False)\n",
        "    print(f\"\\n Saved summary â†’ {os.path.join(base_dir, 'Summary.xlsx')}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# MAIN\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading datasets...\")\n",
        "\n",
        "    src_signals, src_labels, src_rates = load_cwru_signals(SOURCE_KEY)\n",
        "    tgt_signals, tgt_labels, tgt_rates = load_paderborn_signals(TARGET_COND)\n",
        "\n",
        "    print(\"Running 5-Fold Cross-Domain Training (NO DA)...\")\n",
        "    train_no_da_kfold(src_signals, src_labels, tgt_signals, tgt_labels,\n",
        "                      src_rates, tgt_rates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOq1quWavwsC",
        "outputId": "98ecb9cc-2c81-43ae-cc49-e3a4f01bcd1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Running 5-Fold Cross-Domain Training (NO DA)...\n",
            "Source samples: 2264\n",
            "Target samples: 432\n",
            "Target test samples: 432\n",
            "\n",
            "========== FOLD 1/5 ==========\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: TrainLoss=0.2299  ValAcc=0.6556\n",
            "Epoch 2: TrainLoss=0.0267  ValAcc=0.6556\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: TrainLoss=0.0179  ValAcc=0.7285\n",
            "Epoch 4: TrainLoss=0.0126  ValAcc=0.1656\n",
            "Epoch 5: TrainLoss=0.0106  ValAcc=0.1656\n",
            "Epoch 6: TrainLoss=0.0089  ValAcc=0.1656\n",
            "Epoch 7: TrainLoss=0.0065  ValAcc=0.1656\n",
            "Epoch 8: TrainLoss=0.0060  ValAcc=0.1656\n",
            "Epoch 9: TrainLoss=0.0053  ValAcc=0.1656\n",
            "Epoch 10: TrainLoss=0.0062  ValAcc=0.1656\n",
            "Epoch 11: TrainLoss=0.0046  ValAcc=0.1656\n",
            "Epoch 12: TrainLoss=0.0033  ValAcc=0.1656\n",
            "Epoch 13: TrainLoss=0.0031  ValAcc=0.1656\n",
            "Epoch 14: TrainLoss=0.0035  ValAcc=0.1656\n",
            "Epoch 15: TrainLoss=0.0026  ValAcc=0.1656\n",
            "Epoch 16: TrainLoss=0.0029  ValAcc=0.1656\n",
            "Epoch 17: TrainLoss=0.0030  ValAcc=0.1656\n",
            "Epoch 18: TrainLoss=0.0022  ValAcc=0.1656\n",
            "Epoch 19: TrainLoss=0.0020  ValAcc=0.1656\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: TrainLoss=0.0017  ValAcc=0.8212\n",
            "Epoch 21: TrainLoss=0.0016  ValAcc=0.8212\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: TrainLoss=0.0021  ValAcc=0.9338\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: TrainLoss=0.0016  ValAcc=1.0000\n",
            "Epoch 24: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 25: TrainLoss=0.0016  ValAcc=1.0000\n",
            "Epoch 26: TrainLoss=0.0014  ValAcc=1.0000\n",
            "Epoch 27: TrainLoss=0.0014  ValAcc=1.0000\n",
            "Epoch 28: TrainLoss=0.0014  ValAcc=1.0000\n",
            "Epoch 29: TrainLoss=0.0015  ValAcc=1.0000\n",
            "Epoch 30: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 31: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 32: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 33: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 34: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 35: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 36: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 37: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 38: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 39: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 40: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 41: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 42: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 43: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 44: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 45: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 46: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 47: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 48: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 49: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 50: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 51: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 52: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 53: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 54: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 55: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 56: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 57: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 58: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 59: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 60: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 61: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 62: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 63: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 64: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 65: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 66: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 67: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 68: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 69: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 70: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 71: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 72: TrainLoss=0.0003  ValAcc=1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 2/5 ==========\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: TrainLoss=0.3400  ValAcc=0.1567\n",
            "Epoch 2: TrainLoss=0.0473  ValAcc=0.1567\n",
            "Epoch 3: TrainLoss=0.0244  ValAcc=0.1567\n",
            "Epoch 4: TrainLoss=0.0159  ValAcc=0.1567\n",
            "Epoch 5: TrainLoss=0.0113  ValAcc=0.1567\n",
            "Epoch 6: TrainLoss=0.0096  ValAcc=0.1567\n",
            "Epoch 7: TrainLoss=0.0090  ValAcc=0.1567\n",
            "Epoch 8: TrainLoss=0.0068  ValAcc=0.1567\n",
            "Epoch 9: TrainLoss=0.0056  ValAcc=0.1567\n",
            "Epoch 10: TrainLoss=0.0048  ValAcc=0.0971\n",
            "Epoch 11: TrainLoss=0.0043  ValAcc=0.0088\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: TrainLoss=0.0036  ValAcc=0.1678\n",
            "Epoch 13: TrainLoss=0.0033  ValAcc=0.1678\n",
            "Epoch 14: TrainLoss=0.0029  ValAcc=0.1678\n",
            "Epoch 15: TrainLoss=0.0026  ValAcc=0.1678\n",
            "Epoch 16: TrainLoss=0.0023  ValAcc=0.1678\n",
            "Epoch 17: TrainLoss=0.0029  ValAcc=0.1678\n",
            "Epoch 18: TrainLoss=0.0028  ValAcc=0.1678\n",
            "Epoch 19: TrainLoss=0.0027  ValAcc=0.1678\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: TrainLoss=0.0032  ValAcc=0.1921\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: TrainLoss=0.0019  ValAcc=0.9978\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: TrainLoss=0.0020  ValAcc=1.0000\n",
            "Epoch 23: TrainLoss=0.0016  ValAcc=1.0000\n",
            "Epoch 24: TrainLoss=0.0015  ValAcc=1.0000\n",
            "Epoch 25: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 26: TrainLoss=0.0017  ValAcc=1.0000\n",
            "Epoch 27: TrainLoss=0.0015  ValAcc=1.0000\n",
            "Epoch 28: TrainLoss=0.0017  ValAcc=1.0000\n",
            "Epoch 29: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 30: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 31: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 32: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 33: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 34: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 35: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 36: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 37: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 38: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 39: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 40: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 41: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 42: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 43: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 44: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 45: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 46: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 47: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 48: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 49: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 50: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 51: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 52: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 53: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 54: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 55: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 56: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 57: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 58: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 59: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 60: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 61: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 62: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 63: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 64: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 65: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 66: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 67: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 68: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 69: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 70: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 71: TrainLoss=0.0003  ValAcc=1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 3/5 ==========\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: TrainLoss=0.2055  ValAcc=0.8344\n",
            "Epoch 2: TrainLoss=0.0237  ValAcc=0.8344\n",
            "Epoch 3: TrainLoss=0.0177  ValAcc=0.8344\n",
            "Epoch 4: TrainLoss=0.0113  ValAcc=0.8344\n",
            "Epoch 5: TrainLoss=0.0085  ValAcc=0.8344\n",
            "Epoch 6: TrainLoss=0.0063  ValAcc=0.8344\n",
            "Epoch 7: TrainLoss=0.0057  ValAcc=0.8344\n",
            "Epoch 8: TrainLoss=0.0045  ValAcc=0.8344\n",
            "Epoch 9: TrainLoss=0.0039  ValAcc=0.8344\n",
            "Epoch 10: TrainLoss=0.0036  ValAcc=0.8344\n",
            "Epoch 11: TrainLoss=0.0026  ValAcc=0.8344\n",
            "Epoch 12: TrainLoss=0.0028  ValAcc=0.8344\n",
            "Epoch 13: TrainLoss=0.0029  ValAcc=0.8344\n",
            "Epoch 14: TrainLoss=0.0022  ValAcc=0.8344\n",
            "Epoch 15: TrainLoss=0.0030  ValAcc=0.8344\n",
            "Epoch 16: TrainLoss=0.0022  ValAcc=0.8344\n",
            "Epoch 17: TrainLoss=0.0032  ValAcc=0.8344\n",
            "Epoch 18: TrainLoss=0.0018  ValAcc=0.8344\n",
            "Epoch 19: TrainLoss=0.0013  ValAcc=0.8344\n",
            "Epoch 20: TrainLoss=0.0019  ValAcc=0.8344\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: TrainLoss=0.0013  ValAcc=0.8455\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: TrainLoss=0.0015  ValAcc=0.9978\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 24: TrainLoss=0.0016  ValAcc=1.0000\n",
            "Epoch 25: TrainLoss=0.0015  ValAcc=1.0000\n",
            "Epoch 26: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 27: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 28: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 29: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 30: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 31: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 32: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 33: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 34: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 35: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 36: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 37: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 38: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 39: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 40: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 41: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 42: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 43: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 44: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 45: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 46: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 47: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 48: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 49: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 50: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 51: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 52: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 53: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 54: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 55: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 56: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 57: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 58: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 59: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 60: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 61: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 62: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 63: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 64: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Epoch 65: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 66: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 67: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 68: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 69: TrainLoss=0.0003  ValAcc=1.0000\n",
            "Epoch 70: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 71: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 72: TrainLoss=0.0003  ValAcc=1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73: TrainLoss=0.0004  ValAcc=1.0000\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 4/5 ==========\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: TrainLoss=0.2622  ValAcc=0.6645\n",
            "Epoch 2: TrainLoss=0.0352  ValAcc=0.6645\n",
            "Epoch 3: TrainLoss=0.0199  ValAcc=0.6645\n",
            "Epoch 4: TrainLoss=0.0141  ValAcc=0.6645\n",
            "Epoch 5: TrainLoss=0.0098  ValAcc=0.6645\n",
            "Epoch 6: TrainLoss=0.0082  ValAcc=0.6645\n",
            "Epoch 7: TrainLoss=0.0076  ValAcc=0.6645\n",
            "Epoch 8: TrainLoss=0.0065  ValAcc=0.6645\n",
            "Epoch 9: TrainLoss=0.0049  ValAcc=0.6645\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: TrainLoss=0.0040  ValAcc=0.6689\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: TrainLoss=0.0036  ValAcc=0.8322\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: TrainLoss=0.0041  ValAcc=0.8344\n",
            "Epoch 13: TrainLoss=0.0031  ValAcc=0.8344\n",
            "Epoch 14: TrainLoss=0.0030  ValAcc=0.8344\n",
            "Epoch 15: TrainLoss=0.0028  ValAcc=0.8344\n",
            "Epoch 16: TrainLoss=0.0030  ValAcc=0.8344\n",
            "Epoch 17: TrainLoss=0.0025  ValAcc=0.8344\n",
            "Epoch 18: TrainLoss=0.0023  ValAcc=0.8344\n",
            "Epoch 19: TrainLoss=0.0020  ValAcc=0.8344\n",
            "Epoch 20: TrainLoss=0.0021  ValAcc=0.8344\n",
            "Epoch 21: TrainLoss=0.0024  ValAcc=0.8344\n",
            "Epoch 22: TrainLoss=0.0019  ValAcc=0.8344\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: TrainLoss=0.0016  ValAcc=0.9890\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: TrainLoss=0.0018  ValAcc=1.0000\n",
            "Epoch 25: TrainLoss=0.0014  ValAcc=1.0000\n",
            "Epoch 26: TrainLoss=0.0019  ValAcc=1.0000\n",
            "Epoch 27: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 28: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 29: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 30: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 31: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 32: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 33: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 34: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 35: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 36: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 37: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 38: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 39: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 40: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 41: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 42: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 43: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 44: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 45: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 46: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 47: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 48: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 49: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 50: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 51: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 52: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 53: TrainLoss=0.0009  ValAcc=1.0000\n",
            "Epoch 54: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 55: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 56: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 57: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 58: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 59: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 60: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 61: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 62: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 63: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 64: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 65: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 66: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 67: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 68: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 69: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 70: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 71: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Epoch 72: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 73: TrainLoss=0.0006  ValAcc=1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74: TrainLoss=0.0005  ValAcc=1.0000\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 5/5 ==========\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: TrainLoss=0.5695  ValAcc=0.8252\n",
            "Epoch 2: TrainLoss=0.0481  ValAcc=0.6571\n",
            "Epoch 3: TrainLoss=0.0267  ValAcc=0.6571\n",
            "Epoch 4: TrainLoss=0.0142  ValAcc=0.6571\n",
            "Epoch 5: TrainLoss=0.0122  ValAcc=0.6571\n",
            "Epoch 6: TrainLoss=0.0128  ValAcc=0.6571\n",
            "Epoch 7: TrainLoss=0.0094  ValAcc=0.6571\n",
            "Epoch 8: TrainLoss=0.0097  ValAcc=0.6571\n",
            "Epoch 9: TrainLoss=0.0076  ValAcc=0.7876\n",
            "Epoch 10: TrainLoss=0.0070  ValAcc=0.8230\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: TrainLoss=0.0069  ValAcc=0.9867\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: TrainLoss=0.0065  ValAcc=0.9978\n",
            "Epoch 13: TrainLoss=0.0046  ValAcc=0.9912\n",
            "Epoch 14: TrainLoss=0.0057  ValAcc=0.9668\n",
            "Epoch 15: TrainLoss=0.0045  ValAcc=0.9513\n",
            "Epoch 16: TrainLoss=0.0042  ValAcc=0.9646\n",
            "Epoch 17: TrainLoss=0.0043  ValAcc=0.9757\n",
            "Epoch 18: TrainLoss=0.0027  ValAcc=0.9823\n",
            "Epoch 19: TrainLoss=0.0029  ValAcc=0.9934\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: TrainLoss=0.0033  ValAcc=1.0000\n",
            "Epoch 21: TrainLoss=0.0035  ValAcc=1.0000\n",
            "Epoch 22: TrainLoss=0.0029  ValAcc=1.0000\n",
            "Epoch 23: TrainLoss=0.0026  ValAcc=1.0000\n",
            "Epoch 24: TrainLoss=0.0029  ValAcc=1.0000\n",
            "Epoch 25: TrainLoss=0.0020  ValAcc=1.0000\n",
            "Epoch 26: TrainLoss=0.0029  ValAcc=1.0000\n",
            "Epoch 27: TrainLoss=0.0021  ValAcc=1.0000\n",
            "Epoch 28: TrainLoss=0.0028  ValAcc=1.0000\n",
            "Epoch 29: TrainLoss=0.0023  ValAcc=1.0000\n",
            "Epoch 30: TrainLoss=0.0020  ValAcc=1.0000\n",
            "Epoch 31: TrainLoss=0.0018  ValAcc=1.0000\n",
            "Epoch 32: TrainLoss=0.0020  ValAcc=1.0000\n",
            "Epoch 33: TrainLoss=0.0022  ValAcc=1.0000\n",
            "Epoch 34: TrainLoss=0.0014  ValAcc=1.0000\n",
            "Epoch 35: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 36: TrainLoss=0.0019  ValAcc=1.0000\n",
            "Epoch 37: TrainLoss=0.0014  ValAcc=1.0000\n",
            "Epoch 38: TrainLoss=0.0014  ValAcc=1.0000\n",
            "Epoch 39: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 40: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 41: TrainLoss=0.0015  ValAcc=1.0000\n",
            "Epoch 42: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 43: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 44: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 45: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 46: TrainLoss=0.0013  ValAcc=1.0000\n",
            "Epoch 47: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 48: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 49: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 50: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 51: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 52: TrainLoss=0.0012  ValAcc=1.0000\n",
            "Epoch 53: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 54: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 55: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 56: TrainLoss=0.0011  ValAcc=1.0000\n",
            "Epoch 57: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 58: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 59: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 60: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 61: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 62: TrainLoss=0.0010  ValAcc=1.0000\n",
            "Epoch 63: TrainLoss=0.0008  ValAcc=1.0000\n",
            "Epoch 64: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 65: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 66: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 67: TrainLoss=0.0006  ValAcc=1.0000\n",
            "Epoch 68: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Epoch 69: TrainLoss=0.0008  ValAcc=1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70: TrainLoss=0.0007  ValAcc=1.0000\n",
            "Early stopping.\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
            "\n",
            "Generating explainability for Fold 5...\n",
            "Unique labels in y: [0 1 2]\n",
            "Class names length: 3\n",
            "Class names: ['Healthy', 'OR_Fault', 'IR_Fault']\n",
            "Unique predictions: [0 1 2]\n",
            "Unique labels in y: [0 1 2]\n",
            "Class names length: 3\n",
            "Class names: ['Healthy', 'OR_Fault', 'IR_Fault']\n",
            "Unique predictions: [1 2]\n",
            "\n",
            "================================================================================\n",
            "FINAL CROSS-DOMAIN RESULTS SUMMARY (5-FOLD CROSS-VALIDATION)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š SOURCE DOMAIN - CWRU (Validation Accuracy):\n",
            "   (Model trained on CWRU train set, evaluated on CWRU validation set)\n",
            "--------------------------------------------------------------------------------\n",
            "Fold       Accuracy        Precision       Recall          F1-Score       \n",
            "--------------------------------------------------------------------------------\n",
            "Fold_1     100.00%        100.00%        100.00%        100.00%\n",
            "Fold_2     100.00%        100.00%        100.00%        100.00%\n",
            "Fold_3     100.00%        100.00%        100.00%        100.00%\n",
            "Fold_4     100.00%        100.00%        100.00%        100.00%\n",
            "Fold_5     100.00%        100.00%        100.00%        100.00%\n",
            "--------------------------------------------------------------------------------\n",
            "MEAN       100.00%        100.00%        100.00%        100.00%\n",
            "STD          0.00%          0.00%          0.00%          0.00%\n",
            "\n",
            "ðŸ“Š TARGET DOMAIN - Paderborn (Test Accuracy - CROSS-DOMAIN):\n",
            "   (Model trained on CWRU, evaluated on Paderborn test set)\n",
            "--------------------------------------------------------------------------------\n",
            "Fold       Accuracy        Precision       Recall          F1-Score       \n",
            "--------------------------------------------------------------------------------\n",
            "Fold_1      44.44%         28.33%         44.44%         34.52%\n",
            "Fold_2      33.33%         13.41%         33.33%         19.12%\n",
            "Fold_3      33.33%         16.61%         33.33%         22.17%\n",
            "Fold_4      33.33%         16.67%         33.33%         22.22%\n",
            "Fold_5      25.00%         14.29%         25.00%         18.18%\n",
            "--------------------------------------------------------------------------------\n",
            "MEAN        33.89%         17.86%         33.89%         23.24%\n",
            "STD          6.92%          6.03%          6.92%          6.56%\n",
            "\n",
            "ðŸ“‰ CROSS-DOMAIN TRANSFER PERFORMANCE:\n",
            "--------------------------------------------------------------------------------\n",
            "Source Domain (CWRU) Accuracy:     100.00% Â± 0.00%\n",
            "Target Domain (Paderborn) Accuracy: 33.89% Â± 6.92%\n",
            "Accuracy Drop:                      66.11%\n",
            "Transfer Success Rate:              33.89%\n",
            "F1-Score Drop:                      76.76%\n",
            "================================================================================\n",
            "\n",
            "ðŸ’¡ Interpretation:\n",
            "   - High source accuracy (100.00%) shows model learned CWRU data well\n",
            "   - Lower target accuracy (33.89%) shows domain shift challenge\n",
            "   - The 66.11% drop indicates the need for domain adaptation methods\n",
            "================================================================================\n",
            "\n",
            " Saved summary â†’ 12K3HP_COND1/NoDA_20251203_195410/Summary.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base = \"/content/drive/MyDrive/BearingProject/CWRU\"\n",
        "for folder in os.listdir(base):\n",
        "    full = os.path.join(base, folder)\n",
        "    if os.path.isdir(full):\n",
        "        print(\"----\", folder, \"----\")\n",
        "        print(os.listdir(full))\n"
      ],
      "metadata": {
        "id": "eARq4vY60Fah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}